{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; align-items: center;\">\n",
    "  <h1 style=\"font-size: 4.5em; margin-right: 15px;\">Physics Aware Recurrent Convolutional Neural Network (PARC): Navier-Stokes Demo</h1>\n",
    "  <div>\n",
    "    <img src=\"../img/VIL_logo.png\" width=\"190\" alt=\"Image 1\" style=\"margin-right: 1px;\" />\n",
    "    <img src=\"../img/uva.png\" width=\"190\" alt=\"Image 2\" style=\"margin-right: 1px;\" />\n",
    "    <img src=\"../img/iowa.png\" width=\"190\" alt=\"Image 3\" />\n",
    "  </div>\n",
    "</div>\n",
    "<p>A customizable framework to embed physics in Deep Learning. PARC's formulation is inspired by Advection-Diffusion-Reaction processes and uses an Inductive Bias approach to constrain the Neural Network.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PARC?\n",
    "<p>PARC brings together deep learning with first prinicples physics - its recurrent convolutional core is able to learn complex spatiotemporal patterns, while built-in biases (conservation laws & advection-diffusion operators) ensure every prediction is physically plausible. PARC is constructed in such a manner that is does not need to \"re-learn\" fundemental dynamics - allowing faster training with far less data.</p>\n",
    "\n",
    "<p>PINN (Physics-Informed Neural Networks) exists as an adjacent model to PARC. While PINN may preform decently on generics, it struggles with advection or chaotic/non-linear systems - net leading to higher computational cost. In such situations PARC is deemed the superior model with it being more scalable, efficient, and accurate than other physics-based models.</p>\n",
    "\n",
    "### Goal\n",
    "<p>The goal of this notebook is to walk you through the use cases of PARC via the Navier-Stokes equation. At the end of this notebook you will be presented with visualization in accordance to the equations representing conservation of momentum for fluids and fully describing fluid motion.</p>\n",
    "\n",
    "The notebook will guide you through from start to finish in preparing, training, and modeling a physics-based equation. The notebook will cover the following:\n",
    "\n",
    "- loading and preparinf data for Navier-Stokes\n",
    "- Using PARCv2 Model to learn and predict time evolution of $u(x,t)$\n",
    "- Evaluating the model's preformance and compare predicted resutls to ground truth\n",
    "\n",
    "### Internal General PARC PDE\n",
    "<p>Below is the general form of the partial differential equation that PARCv2 is learning - and its inital boundaries. In the case of the Navier-Stokes equations - we can describe the certain vairables to represent the nessearcy points in which PARC tries to model after the equation.</p>\n",
    "\n",
    "$$\n",
    "\\frac{\\partial X}{\\partial t} = f(X,\\mu) + \\epsilon\n",
    "$$\n",
    "\n",
    "- $X$ is the fields of interest - Temperature, Pressure, Reynolds, Velocities (U & V)\n",
    "- $Î¼$ is the microstructure\n",
    "\n",
    "\n",
    "### Setting Up\n",
    "<p>This document serves as a guide to training a PARC model for the Navier-Stokes equation. Here are the inital steps to take before you begin training your PARC model!<p>\n",
    "\n",
    "Download & Prepare Data:\n",
    "- Download the data from https://zenodo.org/records/13909869 and unzip it.\n",
    "- Extact the data and ensure that it placed in the following directory: PARCtorch/PARCtorch/data\n",
    "- If needed update the paths in the notebook accordlingly (for train_dir, test_dir)\n",
    "\n",
    "Install PARCtorch:\n",
    "- Ensure PARCtorch is installed in your Python Environemnt - view instructions on installation at: https://github.com/baeklab/PARCtorch\n",
    "- Note: Ensure to be in the same environemnt/kernel when continuing in this notebook.\n",
    "\n",
    "Happy Modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Prepare Data\n",
    "PARCtorch comes with a built-in dataset containing Navier-Stokes simulations. The following script will allow you to access the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PARC Data Class: tensor([ 300, 3000,  400,  550,  700,  800, 8000,  950]), traj_varying=True, time_varying=False, space_varying=[False, False]>\n",
      "tensor(300)\n",
      "tensor([ True, False,  True,  True,  True,  True, False,  True])\n",
      "tensor([False, False, False, False,  True,  True, False,  True])\n",
      "0\n",
      "Variable time-independent scalar\n",
      "0\n",
      "Variable time-dependent scalar field\n",
      "torch.Size([8, 39, 128, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ -64.7932, 1491.8637,  -35.3208,    4.4460,   53.8535,  123.9229,\n",
       "        2062.4870,  269.5076], dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from PARCtorch.utils.common import CACHE_DIR\n",
    "\n",
    "from PARCtorch.datasets.utils import download, extract_zip\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, data=None, traj_varying=False, time_varying=False, space_varying=False):\n",
    "        self.traj_varying = traj_varying\n",
    "        self.time_varying = time_varying\n",
    "        self.space_varying = space_varying\n",
    "        self.data = torch.as_tensor(data)\n",
    "        # TODO: assert(len(data.shape) >= traj_varying + time_varying + space_varying)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(self, func, types, args=(), kwargs=None):\n",
    "        # Forward any torch function to self.data\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        args = tuple(a.data if isinstance(a, Data) else a for a in args)\n",
    "        return func(*args, **kwargs)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.data == other\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return self.data != other\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        return self.data < other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        return self.data <= other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.data > other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        return self.data >= other\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.data is None:\n",
    "            return 0\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<PARC Data Class: {self.data}, traj_varying={self.traj_varying}, time_varying={self.time_varying}, space_varying={self.space_varying}>\"\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def rank(self):\n",
    "        if self.data is None:\n",
    "            return -1\n",
    "        \n",
    "        rank = len(self.data.shape) - self.traj_varying - self.time_varying\n",
    "        if isinstance(self.space_varying, list):\n",
    "            rank -= sum(self.space_varying)\n",
    "        else:\n",
    "            rank -= self.space_varying\n",
    "\n",
    "        return rank\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def type(self):\n",
    "        type_str = ''\n",
    "        if self.traj_varying:\n",
    "            type_str = 'Variable'\n",
    "        else:\n",
    "            type_str = 'Constant'\n",
    "        \n",
    "        if self.time_varying:\n",
    "            type_str += ' time-dependent'\n",
    "        else:\n",
    "            type_str += ' time-independent'\n",
    "    \n",
    "        rank = self.rank\n",
    "        if rank == 0:\n",
    "            type_str += ' scalar'\n",
    "        elif rank == 1:\n",
    "            type_str += ' vector'\n",
    "        else:\n",
    "            type_str += f' rank-{rank} tensor'\n",
    "\n",
    "        if any(self.space_varying):\n",
    "            type_str += ' field'\n",
    "\n",
    "        return type_str\n",
    "            \n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Data is \n",
    "    '''\n",
    "    url = \"https://zenodo.org/records/13909869/files/NavierStokes.zip?download=1\"\n",
    "    def __init__(\n",
    "            self,\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        # TODO: https://github.com/baeklab/PARCtorch/blob/main/tests/conftest.py\n",
    "        self.dataset_name = ''              # dataset name\n",
    "        self.grid_type = 'cartesian'        # type of grid. Can only be 'cartesian' at the moment. TODO: 'mesh'\n",
    "        self.n_spatial_dims = 1             # spatial dimensions\n",
    "        self.dimensions = {}\n",
    "        self.boundary_conditions = {}\n",
    "        self.constants = {}         # physics parameters that are constant across all samples\n",
    "        self.data = {}              # \n",
    "\n",
    "        self.filelist = {}\n",
    "                \n",
    "\n",
    "        # Set up data directory\n",
    "        data_dir = None\n",
    "        if data_dir is None:\n",
    "            data_dir = os.path.join(CACHE_DIR, 'datasets', 'NavierStokes')\n",
    "        self.data_dir = data_dir\n",
    "        self.zip_dir = os.path.join(self.data_dir, 'NavierStokes.zip')\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "\n",
    "        # Download and unzip data if it doesn't exist already.\n",
    "        self.download(force=False)\n",
    "\n",
    "        # Read data\n",
    "        self.load()\n",
    "\n",
    "        \n",
    "        # TODO: Lazy loading (Future; low priority)\n",
    "        pass\n",
    "\n",
    "    # number of trajectories or simulation instances\n",
    "    @property\n",
    "    def n_trajectories(self):\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def download(self, force=False):\n",
    "        if not os.path.exists(self.zip_dir):\n",
    "            download(self.url, self.zip_dir)\n",
    "            extract_zip(self.zip_dir, self.data_dir)\n",
    "\n",
    "    def load(self, data_dir=None):\n",
    "        if data_dir is None:\n",
    "            data_dir = self.data_dir\n",
    "\n",
    "        filelist = Path(data_dir).glob('test/*.npy')\n",
    "        \n",
    "        \n",
    "        self.dataset_name = 'Navier Stokes'\n",
    "        self.grid_type = 'cartesian'\n",
    "        self.n_spatial_dims = 2\n",
    "        self.dimensions = {\n",
    "            'time': Data(torch.linspace(0,1,39)),\n",
    "            'x': Data(torch.linspace(0,1,256)),\n",
    "            'y': Data(torch.linspace(0,1,128)),\n",
    "        }\n",
    "        # self.constants = {}\n",
    "        # self.boundary_conditions = {}\n",
    "\n",
    "        self.filelist = []\n",
    "        reynolds_numbers = []\n",
    "        pressure_fields = []\n",
    "        velocity_fields = []\n",
    "        for filepath in filelist:\n",
    "            reynolds_numbers.append( int(filepath.stem.split('_')[-1]) )\n",
    "            fields = np.load(filepath)\n",
    "            pressure_fields.append(fields[:,0,:,:])\n",
    "            velocity_fields.append(fields[:,2:,:,:])\n",
    "            self.filelist.append(filepath)\n",
    "    \n",
    "        self.data = {\n",
    "            'Reynolds Number': Data(\n",
    "                torch.tensor(reynolds_numbers),\n",
    "                traj_varying=True,\n",
    "                time_varying=False,\n",
    "                space_varying=[False, False],\n",
    "            ),\n",
    "            'Pressure': Data(\n",
    "                torch.tensor(pressure_fields),\n",
    "                traj_varying=True,\n",
    "                time_varying=True,\n",
    "                space_varying=[True, True],\n",
    "            ),\n",
    "            'Velocity': Data(\n",
    "                torch.tensor(velocity_fields),\n",
    "                traj_varying=True,\n",
    "                time_varying=True,\n",
    "                space_varying=[True, True],\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "    # def __getitem__(self):\n",
    "    \n",
    "    # def __len__(self):\n",
    "\n",
    "    # def normalize(self):\n",
    "\n",
    "    # train_ds = ds.filter( torch.reduce_sum(ds.data['morphology'], dim=[1,2]) < 100 )\n",
    "\n",
    "    def filter(self, mask):\n",
    "        '''\n",
    "        train = ds.filter( ds[ds.physics_parameters[0]] < 1000 )\n",
    "        test = ds.filter( ds.meta.physics_parameters[0] >= 1000 )\n",
    "        '''\n",
    "        assert( len(mask.shape) == 1 )    # TODO: error message\n",
    "        assert( len(mask) == self.n_trajectories ) # TODO: error message\n",
    "\n",
    "        indices = torch.nonzero(mask)\n",
    "        ds = Dataset()  # TODO: Copy constructor\n",
    "        ds.filelist = self.filelist(indices)   # TODO: Deep copy?\n",
    "        ds.data = self.data[indices]   # TODO: Deep copy?\n",
    "        ds.grid_type = self.grid_type\n",
    "        ds.n_spatial_dims = self.n_spatial_dims\n",
    "        ds.data_dir = self.data_dir\n",
    "        ds.dimensions = self.dimensions\n",
    "        ds.boundary_conditions = self.boundary_conditions\n",
    "        ds.constants = self.constants\n",
    "        # ds.dataset_name\n",
    "                \n",
    "        \n",
    "\n",
    "ds = Dataset()\n",
    "print(ds.data['Reynolds Number'])\n",
    "print(ds.data['Reynolds Number'][0])\n",
    "print(ds.data['Reynolds Number'] < 1000)\n",
    "print(ds.data['Reynolds Number'] < torch.tensor([1,2,3,4,1000,1000,1000,1000]))\n",
    "print(ds.data['Reynolds Number'].rank)\n",
    "print(ds.data['Reynolds Number'].type)\n",
    "\n",
    "print(ds.data['Pressure'].rank)\n",
    "print(ds.data['Pressure'].type)\n",
    "print(ds.data['Pressure'].shape)\n",
    "torch.mean(ds.data['Pressure'], dim=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 39, 128, 256])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPressure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m, dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Data' object has no attribute 'value'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PARCtorch.data.dataset import GenericPhysicsDataset\n",
    "from PARCtorch.utils.common import CACHE_DIR\n",
    "\n",
    "from PARCtorch.datasets.utils import download, extract_zip\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "class NavierStokes(GenericPhysicsDataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Navier-Stokes data set. TODO: More details\n",
    "    \n",
    "    Args:\n",
    "        split: Expected data split. Can be `train`, `test` TODO\n",
    "        data_dir: Directory to read/write data. Defaults to None, in which case,\n",
    "                  data will be stored in `CACHE_DIR/datasets/NavierStokes`.\n",
    "                  If the data does not exist in the specified directory,\n",
    "                  it will be automatically downloaded.\n",
    "        future_steps: Number of timesteps in the future the model will predict.\n",
    "                      Must be between 1 (single step prediction) and TODO (default).\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"https://zenodo.org/records/13909869/files/NavierStokes.zip?download=1\"\n",
    "\n",
    "    def __init__(\n",
    "        self, split=None, data_dir=None, future_steps=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up data directory\n",
    "        if data_dir is None:\n",
    "            data_dir = os.path.join(CACHE_DIR, 'datasets', 'NavierStokes')\n",
    "        self.data_dir = data_dir\n",
    "        self.zip_dir = os.path.join(self.data_dir, 'NavierStokes.zip')\n",
    "\n",
    "        if not os.path.exists(self.data_dir):\n",
    "            os.makedirs(self.data_dir)\n",
    "\n",
    "        # Download and unzip data if it doesn't exist already.\n",
    "        self.download(force=False)\n",
    "\n",
    "        from pathlib import Path\n",
    "        filelist = Path(self.data_dir).glob('*.*')\n",
    "\n",
    "\n",
    "    def download(self, force=False):\n",
    "        if not os.path.exists(self.zip_dir):\n",
    "            download(self.url, self.zip_dir)\n",
    "            extract_zip(self.zip_dir, self.data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GenericPhysicsDataset.__init__() missing 1 required positional argument: 'data_dirs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mNavierStokes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m, in \u001b[0;36mNavierStokes.__init__\u001b[1;34m(self, split, data_dir, future_steps)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, future_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     28\u001b[0m ):\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Set up data directory\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: GenericPhysicsDataset.__init__() missing 1 required positional argument: 'data_dirs'"
     ]
    }
   ],
   "source": [
    "data = NavierStokes()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PARCtorch' has no attribute 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPARCtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mparc\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mparc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mNavierStokes()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'PARCtorch' has no attribute 'datasets'"
     ]
    }
   ],
   "source": [
    "import PARCtorch as parc\n",
    "parc.datasets.NavierStokes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Data Normalization\n",
    "<p>In this first step we preform min-max normalization - <code>compute_min_max</code> is used to calculate all minimum and maximum values across all provided datasets. This is an important step as normalization improves the predictive accuracy of the model by making sure data falls within a standardized numerical range.</p>\n",
    "\n",
    "<p>After running this cell the file <strong>ns_min_max.json</strong> will contain parameters to consistently scale data during training and prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PARCtorch.data.normalization import compute_min_max\n",
    "\n",
    "# Define data directories\n",
    "train_dir = Path(\"../data/navier_stokes/train\")\n",
    "test_dir = Path(\"../data/navier_stokes/test\")\n",
    "min_max_file = train_dir.parent / \"ns_min_max.json\"\n",
    "\n",
    "compute_min_max([train_dir, test_dir], min_max_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for Training\n",
    "<p> We next create a <strong>DataLoader</strong> for training. The DataLoader in PyTorch is a crucial utility that facilitates efficient data handling for training and evaluating machine learning models. It abstracts the process of fetching, batching, and shuffling data, ensuring that the model is fed with properly formatted inputs in an optimal way. Specifically, it helps with:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Batching:</strong> Splitting large datasets into smaller, manageable batches to avoid memory overload and enable parallel processing.</li>\n",
    "    <li><strong>Shuffling:</strong> Randomly ordering data to prevent the model from learning patterns related to the sequence of data (particularly important in training to reduce overfitting).</li>\n",
    "    <li><strong>Parallel Loading:</strong> It allows the data to be loaded asynchronously using multiple workers, speeding up the training process by loading the next batch while the current one is being processed by the model.</li>\n",
    "    <li><strong>Custom Collation:</strong> The <code>collate_fn</code> allows customization of how batches are combined, which is essential for complex datasets that require specific handling.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now import the utilities\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Now import the utilities\n",
    "from PARCtorch.data.dataset import (\n",
    "    GenericPhysicsDataset,\n",
    "    custom_collate_fn,\n",
    "    InitialConditionDataset,\n",
    "    initial_condition_collate_fn,\n",
    ")\n",
    "from PARCtorch.utilities.viz import (\n",
    "    visualize_channels,\n",
    "    save_gifs_with_ground_truth,\n",
    ")\n",
    "future_steps = 1\n",
    "batch_size = 8\n",
    "\n",
    "# Initialize the dataset\n",
    "train_dataset = GenericPhysicsDataset(\n",
    "    data_dirs=[train_dir],\n",
    "    future_steps=future_steps,\n",
    "    min_max_path=min_max_file,\n",
    ")\n",
    "\n",
    "# Create DataLoader for training dataset\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data - Check it was Loaded Properly\n",
    "<p> We are now going to visualize our data to verify if the correct data is loading and formatting. In the cell below we are iterating over the previously defined DataLoader and fetching one batch of data. The batch is then split up into the following variables:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><code>ic</code>: Inital conditions - the initial state of the physical system</li>\n",
    "    <li><code>t0</code>: Starting time-step </li>\n",
    "    <li><code>t1</code>: Ending time-step </li>\n",
    "    <li><code>target</code>: Ground truth future states</li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a batch and visualize\n",
    "for batch in train_loader:\n",
    "    ic, t0, t1, target = batch\n",
    "    channel_names = [\n",
    "        \"Pressure (P)\",\n",
    "        \"Reynolds (R)\",\n",
    "        \"Velocity U\",\n",
    "        \"Velocity V\",\n",
    "    ]\n",
    "    custom_cmaps = [\"seismic\", \"seismic\", \"seismic\", \"seismic\"]\n",
    "\n",
    "    visualize_channels(\n",
    "        ic,\n",
    "        t0,\n",
    "        t1,\n",
    "        target,\n",
    "        channel_names=channel_names,\n",
    "        channel_cmaps=custom_cmaps,\n",
    "    )\n",
    "    break  # Visualize one batch for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Build Your PARC Model</h2>\n",
    "<p>In this section, we are constructing a <strong>PARCv2 model</strong>, which is designed to handle spatiotemporal data, such as fluid dynamics simulations. The model leverages various components, including <em>differentiators</em> and <em>integrators</em>, to solve physical equations like Navier-Stokes.</p>\n",
    "\n",
    "<p><strong>Key Components:</strong></p>\n",
    "<ul>\n",
    "    <li><strong>UNet:</strong> The UNet architecture is used for feature extraction, transforming the input data into a higher-dimensional representation. This helps the model capture complex patterns in the physical simulation data.</li>\n",
    "    <li><strong>FiniteDifference:</strong> This differentiator approximates the gradients (or derivatives) of the input data using a finite difference method, which is important for calculating advection and diffusion processes in fluid dynamics.</li>\n",
    "    <li><strong>Heun Integrator:</strong> Heunâs method is an improved version of Euler's method, used here to integrate the equations of motion more accurately over time.</li>\n",
    "    <li><strong>Differentiator:</strong> This module calculates the advection and diffusion terms based on specific channel indices, such as velocity in the x and y directions (u and v).</li>\n",
    "    <li><strong>Integrator:</strong> The integrator applies Heunâs method to combine the differentiated terms and solve the Poisson equation, ensuring the physical constraints of the system are respected.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The model is then wrapped into the <strong>PARCv2</strong> class, which combines the differentiator, integrator, and loss function (<code>L1Loss</code>). Finally, an <code>Adam</code> optimizer is initialized to train the model by adjusting its parameters to minimize the error between predictions and ground truth data.</p>\n",
    "\n",
    "<p>This setup allows the model to learn how to predict future states in complex physical systems by embedding domain-specific knowledge.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PARCtorch.PARCv2 import PARCv2\n",
    "from PARCtorch.differentiator.differentiator import ADRDifferentiator\n",
    "from PARCtorch.differentiator.finitedifference import FiniteDifference\n",
    "from PARCtorch.integrator.integrator import Integrator\n",
    "from PARCtorch.integrator.heun import Heun\n",
    "from PARCtorch.utilities.unet import UNet\n",
    "\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining Navier-Stokes variables:\n",
    "# p = pressure, re = Reynolds number, u = velocity in x-direction, v = velocity in y-direction\n",
    "# Advection (Adv) and Diffusion (Dif) will be calculated on u, v\n",
    "# Poisson equation (Poi) will be calculated on pressure (p)\n",
    "\n",
    "n_fe_features = 128  # Number of features extracted by the UNet\n",
    "\n",
    "# Initialize the UNet architecture for feature extraction\n",
    "unet_ns = UNet(\n",
    "    [64, 64 * 2, 64 * 4, 64 * 8, 64 * 16],  \n",
    "    4,  \n",
    "    n_fe_features,  \n",
    "    up_block_use_concat=[False, True, False, True],  \n",
    "    skip_connection_indices=[2, 0],  \n",
    ")\n",
    "\n",
    "# Initialize finite difference method for numerical differentiation\n",
    "right_diff = FiniteDifference(padding_mode=\"replicate\").cuda()  # Use replication padding to handle boundary conditions\n",
    "\n",
    "# Initialize Heun's method for numerical integration\n",
    "heun_int = Heun().cuda() \n",
    "\n",
    "# Create the Differentiator, responsible for calculating advection and diffusion\n",
    "diff_ns = ADRDifferentiator(\n",
    "    2,  \n",
    "    n_fe_features,  \n",
    "    [2, 3],  \n",
    "    [2, 3],  \n",
    "    unet_ns,  \n",
    "    \"constant\",  \n",
    "    right_diff,  \n",
    "    False \n",
    ").cuda()\n",
    "\n",
    "# Create the Integrator, responsible for solving the Poisson equation and performing integration\n",
    "ns_int = Integrator(\n",
    "    True,  \n",
    "    [(0, 2, 3, 1)],  \n",
    "    heun_int,  \n",
    "    [None, None, None, None],  \n",
    "    \"constant\",  \n",
    "    right_diff,  \n",
    ").cuda()\n",
    "\n",
    "# Define the loss function (L1 Loss is typically used for regression tasks)\n",
    "criterion = torch.nn.L1Loss().cuda()\n",
    "\n",
    "# Initialize the PARCv2 model with the differentiator, integrator, and loss function\n",
    "model = PARCv2(diff_ns, ns_int, criterion).cuda()\n",
    "\n",
    "# Set up the optimizer (Adam is a popular choice for adaptive learning rate optimization)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model \n",
    "<p>Now, we will be training the PARC model using the training data and the paramaters we have setup above. Here we iterate over the dataset given by <code>train_loader</code> batch-by-batch and in each iteration we: </p>\n",
    "\n",
    "<ul>\n",
    "    <li>Compute model predictions</li>\n",
    "    <li>Calculate error loss between predictions and ground truth values via the <code>criterion</code> loss function </li>\n",
    "    <li>Update the model parameters based on error via the <code>optimizer</code></li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PARCtorch.train import train_model\n",
    "\n",
    "# Example usage:\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=1,\n",
    "    save_dir=train_dir.parent,\n",
    "    app=\"ns\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "<p> Post training, we can now load the previously trained model weights into our PARC model for evalutation. Here, we locate the model weights and load them into the exisiting PARC model architecture in order to prep for visualization.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PARCtorch.utilities.load import load_model_weights\n",
    "\n",
    "# Example Usage:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_weights_path = (\n",
    "    train_dir.parent / \"model.pth\"  # Replace with your path\n",
    ")\n",
    "model = load_model_weights(model, model_weights_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence DataLoader\n",
    "<p>We now create a sequence DataLoader designed to test the dataset - providing the initial conditions as <code>t=0</code> for generating predictiond over multiple timesteps. Using <code>DataLoader</code> we wrap the dataset for efficent batching and processing.</p>\n",
    "\n",
    "<p>We also create the Initial Condition Dataset, which loades the initial state for the physical system, the previously computed normalization parameters, and prepares the dataset to be fed into the model. We also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "future_steps = 10\n",
    "seq_dataset = InitialConditionDataset(\n",
    "    data_dirs=[test_dir],\n",
    "    future_steps=future_steps,\n",
    "    min_max_path=min_max_file,\n",
    ")\n",
    "\n",
    "# Create DataLoader for training dataset\n",
    "seq_loader = DataLoader(\n",
    "    seq_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=initial_condition_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Loader\n",
    "<p>Here, we set up the Ground Truth Loader to provide a comparison of the actual future states of the system against the models predictions. Similar set up to previous <code>DataLoader</code> setups in this notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset\n",
    "gt_dataset = GenericPhysicsDataset(\n",
    "    data_dirs=[test_dir],\n",
    "    future_steps=future_steps,\n",
    "    min_max_path=min_max_file,\n",
    ")\n",
    "\n",
    "# Create DataLoader for training dataset\n",
    "gt_loader = DataLoader(\n",
    "    gt_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    collate_fn=custom_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Results\n",
    "<p> We now finially visualize our results. Here, we run inferences using thr trained PARC model's predictions against the ground truth. Visualization gifs of the predictions and ground truth are outputted - just runs on the first batch for demo.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define channel names and colormaps\n",
    "channels = [\"pressure\", \"Reynolds\", \"u\", \"v\"]  # Adjust as per your data\n",
    "cmaps = [\n",
    "    \"viridis\",\n",
    "    \"plasma\",\n",
    "    \"inferno\",\n",
    "    \"magma\",\n",
    "]  # Adjust as per your preference\n",
    "\n",
    "# Iterate through both DataLoaders simultaneously\n",
    "for seq_batch, test_batch in zip(seq_loader, gt_loader):\n",
    "    # Extract data from initial condition loader\n",
    "    ic, t0, t1, _ = (\n",
    "        seq_batch  # Shape: [batch_size, channels, height, width], scalar, tensor, _\n",
    "    )\n",
    "\n",
    "    # Extract data from ground truth loader\n",
    "    gt_ic, gt_t0, gt_t1, ground_truth = (\n",
    "        test_batch  # ground_truth shape: [timesteps, batch_size, channels, height, width]\n",
    "    )\n",
    "\n",
    "    # Move data to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ic = ic.to(device)\n",
    "    t0 = t0.to(device)\n",
    "    t1 = t1.to(device)\n",
    "    ground_truth = ground_truth.to(device)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    with torch.no_grad():\n",
    "        predictions = model(\n",
    "            ic, t0, t1\n",
    "        )  # Shape: [future_steps, batch_size, channels, height, width]\n",
    "\n",
    "    print(\"Predictions shape:\", predictions.shape)\n",
    "    print(\n",
    "        \"Sample prediction for timestep 1:\", predictions[:, 0, :, :, :].shape\n",
    "    )\n",
    "\n",
    "    # If you want to visualize more samples in the batch, loop through batch indices\n",
    "    # For example, to visualize all samples in the batch:\n",
    "    for batch_idx in range(ic.size(0)):\n",
    "        save_gifs_with_ground_truth(\n",
    "            predictions=predictions,\n",
    "            ground_truth=ground_truth,\n",
    "            channels=channels,\n",
    "            cmaps=cmaps,\n",
    "            filename_prefix=f\"comparison_batch{batch_idx}\",\n",
    "            interval=0.2,\n",
    "            batch_idx=batch_idx,\n",
    "        )\n",
    "        break\n",
    "\n",
    "    break  # Remove this if you want to process the entire dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
